{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as td\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import cv2\n",
    "import torchvision\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "def load_image_data(input_path, max_size):\n",
    "    # load a list of image file paths\n",
    "    data_list = os.listdir(input_path)\n",
    "    # create a list of images\n",
    "    images = []\n",
    "    for i, l in enumerate(data_list):\n",
    "        if i > max_size:\n",
    "            break\n",
    "        p = input_path + l\n",
    "        im = cv2.imread(p)\n",
    "        images.append(im)\n",
    "    return images\n",
    "\n",
    "# Build Data Loader\n",
    "def set_image_data(x_path, c_path, batch_size=100, max_size=100000):\n",
    "    x_images = load_image_data(x_path, max_size)\n",
    "    c_images = load_image_data(c_path, max_size)\n",
    "\n",
    "    # list to torch tensor\n",
    "    # each element is divided by 255 to float\n",
    "    # axis change from (n_image, height, width, channel) to (n_image, channel, height, width)\n",
    "    x_tensor = torch.from_numpy((np.array(x_images) / 255.).astype(np.float32)).to(device)\n",
    "    x_tensor = x_tensor.permute((0,3,1,2))\n",
    "    c_tensor = torch.from_numpy((np.array(c_images) / 255.).astype(np.float32)).to(device)\n",
    "    c_tensor = c_tensor.permute((0,3,1,2))\n",
    "\n",
    "    # output data set\n",
    "    image_set = td.TensorDataset(x_tensor, c_tensor) # train data and the label\n",
    "    dataloader = td.DataLoader(dataset=image_set, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-VAE Model components\n",
    "class EncoderModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride, kernel, pad):\n",
    "        super().__init__()\n",
    "        # 2D convolution: kernel=1->filter size=[1,1], pad=0->padding size=0 (NONE padding),pad=1->ZERO padding for size 1 (for both edges)\n",
    "        # TO BE REVIEWED: Can Pooling give some improvement to the face generation??\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=kernel, padding=pad, stride=stride)\n",
    "        # Batch normalisation\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        # Relu activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, color_channels, pooling_kernels, n_neurons_in_middle_layer):\n",
    "        self.n_neurons_in_middle_layer = n_neurons_in_middle_layer\n",
    "        super().__init__()\n",
    "        # (N,3,50,100)->(N,32,50,100)\n",
    "        self.bottle = EncoderModule(color_channels, 32, stride=1, kernel=1, pad=0)\n",
    "        # (N,32,50,100)->(N,64,50,100)\n",
    "        self.m1 = EncoderModule(32, 64, stride=1, kernel=3, pad=1)\n",
    "        # (N,64,50,100)->(N,128,12,24) with stride size 4. TO BE REVIEWED: pad=0 or p\n",
    "        self.m2 = EncoderModule(64, 128, stride=4, kernel=[3,5], pad=0)\n",
    "        # (N,128,12,24)->(N,256,4,4) with stride size [2,4] TO BE REVIEWED: pad=0 or p\n",
    "        self.m3 = EncoderModule(128, 256, stride=[2,4], kernel=[5,9], pad=0)\n",
    "    def forward(self, x):\n",
    "        out = self.m3(self.m2(self.m1(self.bottle(x))))\n",
    "        return out.view(-1, self.n_neurons_in_middle_layer)\n",
    "\n",
    "class DecoderModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride, activation=\"relu\", kernel=1):\n",
    "        super().__init__()\n",
    "        # 2D Deconvolution\n",
    "        self.convt = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=kernel, stride=stride)\n",
    "        # Batch renormalisation\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        # ReLu or Sigmoid activation\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.bn(self.convt(x)))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, color_channels, pooling_kernels, decoder_input_size):\n",
    "        self.decoder_input_size = decoder_input_size\n",
    "        self.in_channels = in_channels\n",
    "        super().__init__()\n",
    "        # (N,128,12,24)<-(N,256,4,4) (in_channels=256 for VAE, 512 for CVAE)\n",
    "        self.m1 = DecoderModule(self.in_channels, 128, stride=[3,6], kernel=[3,6])\n",
    "        # (N,64,50,100)<-(N,128,12,24)\n",
    "        self.m2 = DecoderModule(128, 64, stride=[4,4], kernel=[6,8])\n",
    "        # (N,32,50,100)<-(N,64,50,100)\n",
    "        self.m3 = DecoderModule(64, 32, stride=1, kernel=1)\n",
    "        # (N,3,50,100)<-(N,32,50,100)\n",
    "        self.bottle = DecoderModule(32, color_channels, stride=1, activation=\"sigmoid\", kernel=1)\n",
    "    def forward(self, x):\n",
    "        # support 2 cases\n",
    "        # VAE: input size (N,256*4*4) ->(N,256,4,4)\n",
    "        # CVAE: input size (N,2*256*4*4) ->(N,512,4,4)\n",
    "        out = x.view(-1, self.in_channels, self.decoder_input_size, self.decoder_input_size)\n",
    "        out = self.m3(self.m2(self.m1(out)))\n",
    "        return self.bottle(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C)VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, conditional):\n",
    "        self.device = device\n",
    "        self.conditional = conditional # if true, CVAE\n",
    "        super().__init__()\n",
    "        \n",
    "        # Common feature in Encoder/Decoder (De)Convolution layer\n",
    "        color_channels = 3\n",
    "        pooling_kernel = [1, 1]\n",
    "        encoder_output_size = 4 # Given by the Encoder architecture\n",
    "        encoder_1d_output_size = 256 * encoder_output_size * encoder_output_size\n",
    "\n",
    "        # Encoder for X (lower-half face)\n",
    "        self.encoder_x = Encoder(color_channels, pooling_kernel, encoder_1d_output_size)        \n",
    "        # Encoder for C (Upper-half face, \"label\")\n",
    "        if self.conditional:\n",
    "            self.encoder_c = Encoder(color_channels, pooling_kernel, encoder_1d_output_size) # ALMOST 100% SURE ABOUT THIS SETTING, BUT TO BE REVIEWED\n",
    "\n",
    "        # Latent Features\n",
    "        # output size [N,256,4,4]\n",
    "        self.n_latent_features = 64 # dimension of z\n",
    "        n_neurons_middle_layer = (2 if self.conditional else 1) * encoder_1d_output_size # we double if CVAE\n",
    "        \n",
    "        # for encoder X\n",
    "        self.fc1 = nn.Linear(n_neurons_middle_layer, self.n_latent_features)\n",
    "        self.fc2 = nn.Linear(n_neurons_middle_layer, self.n_latent_features)\n",
    "        self.fc3 = nn.Linear(self.n_latent_features, encoder_1d_output_size)\n",
    "\n",
    "        # for encoder C\n",
    "        if self.conditional:\n",
    "            self.fc4 = nn.Linear(encoder_1d_output_size, self.n_latent_features) \n",
    "            self.fc5 = nn.Linear(encoder_1d_output_size, self.n_latent_features)\n",
    "            self.fc6 = nn.Linear(self.n_latent_features, encoder_1d_output_size)\n",
    "\n",
    "        # Decoder for X (Labels should be taken into account also for this decoder stage...)\n",
    "        self.decoder_x = Decoder(256*(2 if self.conditional else 1), color_channels, pooling_kernel, encoder_output_size)\n",
    "        # Decoder for C (Upper-half face, \"label\")\n",
    "        if self.conditional:\n",
    "            self.decoder_c = Decoder(256, color_channels, pooling_kernel, encoder_output_size)\n",
    "\n",
    "    # Reparametrisation trick\n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn(*mu.size()).to(self.device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    # From the encoder_x output, Output the latent vector \"zx\" \n",
    "    def _bottleneck_x(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    # From the encoder_c output, Output the latent vector \"zc\" \n",
    "    def _bottleneck_c(self, h):\n",
    "        mu, logvar = self.fc4(h), self.fc5(h)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    # Sampling lower-half face (given an input upper-half face image)\n",
    "    def sampling(self, c=None, r=None):\n",
    "        # assume latent features space ~ N(0, 1)\n",
    "        n = c.size(0) if self.conditional else 64 # number of faces\n",
    "        if r==None:\n",
    "            zx = torch.randn(n, self.n_latent_features).to(self.device)\n",
    "        else:\n",
    "            zx = torch.full((n,self.n_latent_features), fill_value=r).to(self.device)\n",
    "        zx = self.fc3(zx) # 256 * encode_out_size(4)* encode_out_size(4)\n",
    "        \n",
    "        # concat\n",
    "        if self.conditional:\n",
    "            hc = self.encoder_c(c) # 256 * encode_out_size(4)* encode_out_size(4)\n",
    "            z = torch.cat((zx, hc), dim=-1) # 2* 256 * encode_out_size(4)* encode_out_size(4)\n",
    "        else:\n",
    "            z = zx\n",
    "        # decode\n",
    "        dx = self.decoder_x(z)\n",
    "        d = torch.cat((c,dx), dim=2)\n",
    "        return d\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        # Encoder for X and for C\n",
    "        # output size [N,256*4*4]\n",
    "        hx = self.encoder_x(x)\n",
    "        if self.conditional:\n",
    "            hc = self.encoder_c(c)\n",
    "        # concat if CVAE\n",
    "        if self.conditional:\n",
    "            h = torch.cat((hx, hc), dim=-1)\n",
    "        else:\n",
    "            h = hx\n",
    "        # Bottle-neck\n",
    "        # z,mu,logvar size [N,256*4*4]->[N,64]\n",
    "        zx, mu_x, logvar_x = self._bottleneck_x(h)\n",
    "        if self.conditional:\n",
    "            zc, mu_c, logvar_c = self._bottleneck_c(hc)\n",
    "            \n",
    "        # decoder\n",
    "        # output size [N,64]->[N,256*4*4]\n",
    "        zx = self.fc3(zx)\n",
    "        if self.conditional:\n",
    "            zx = torch.cat((zx, hc), dim=-1) # 2* 256 * encode_out_size(4)* encode_out_size(4)\n",
    "            zc = self.fc6(zc)\n",
    "            \n",
    "        dx = self.decoder_x(zx)\n",
    "        if self.conditional:\n",
    "            dc = self.decoder_c(zc)\n",
    "            d = torch.cat((dc,dx), dim=2)\n",
    "            mu = torch.sqrt(mu_x.pow(2) + mu_c.pow(2))\n",
    "            logvar = torch.sqrt(logvar_x.pow(2) + logvar_c.pow(2))\n",
    "        else:\n",
    "            d = dx\n",
    "            mu = mu_x\n",
    "            logvar = logvar_x\n",
    "        return d, mu, logvar\n",
    "\n",
    "    # Model loss function\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        # https://arxiv.org/abs/1312.6114 (Appendix B)\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)        \n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "\n",
    "    def init_model(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self = self.cuda()\n",
    "            torch.backends.cudnn.benchmark=True\n",
    "        self.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Save (C)VAE Model\n",
    "def train_and_save(data_loader, model_path, csv_log_path, nepoch=20, nReportFreq=20, conditional=True):\n",
    "    # Init the model\n",
    "    cvae = VAE(conditional)\n",
    "    cvae.init_model()\n",
    "    # Training mode\n",
    "    cvae.train()\n",
    "    history = []\n",
    "    for epoch in range(nepoch):\n",
    "        train_loss = 0\n",
    "        samples_cnt = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # Extract input (lower-half face) and label (upper-half face)\n",
    "            inputs, labels = data\n",
    "            # Define them as torch Variable (auto-gradient will be applied on)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            # init the optimiser (FORGOT why we initialise the grad at the beginning of each epoch)\n",
    "            cvae.optimizer.zero_grad()\n",
    "            # forward\n",
    "            recon_batch, mu, logvar = cvae.forward(inputs, labels)\n",
    "            # evaluate loss\n",
    "            inputs_full = torch.cat((labels, inputs), dim=2)\n",
    "            loss = cvae.loss_function(recon_batch, inputs_full, mu, logvar)\n",
    "            # back prop and update the weights\n",
    "            loss.backward()\n",
    "            cvae.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            samples_cnt += inputs.size(0)\n",
    "\n",
    "            if i%nReportFreq == 0:\n",
    "                history.append([epoch, i, data_loader.batch_size*i, train_loss / samples_cnt])\n",
    "                print('#Epoch: {:}, #Batch: {:}, #Pics:{:} Loss: {:.2f}'.format(epoch, i, data_loader.batch_size*i, train_loss / samples_cnt))\n",
    "\n",
    "    # save model\n",
    "    state_dict = cvae.state_dict()\n",
    "    torch.save(state_dict, model_path)\n",
    "    # save log\n",
    "    if csv_log_path != None:\n",
    "        col = [\"#Epoch\", \"#Batch\", \"#Pic\", \"Loss\"]\n",
    "        df_history = pd.DataFrame(data=history, columns=col)\n",
    "        df_history.to_csv(csv_log_path)\n",
    "\n",
    "# Testing\n",
    "def test_and_save(dataloader, model_path, reconstr_path, csv_log_path, nrow=10, nReportFreq=20, conditional=True):\n",
    "    # Load the trained model\n",
    "    cvae = VAE(conditional)\n",
    "    cvae.init_model()\n",
    "    state_dict = torch.load(model_path)\n",
    "    cvae.load_state_dict(state_dict)\n",
    "\n",
    "    # Evaluation\n",
    "    cvae.eval()\n",
    "    val_loss = 0\n",
    "    samples_cnt = 0\n",
    "    history = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            # Extract input (lower-half face) and label (upper-half face)\n",
    "            inputs, labels = data\n",
    "            # Define them as torch Variable (auto-gradient will be applied on)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            recon_batch, mu, logvar = cvae.forward(inputs, labels)\n",
    "            inputs_full = torch.cat((labels, inputs), dim=2)\n",
    "            val_loss += cvae.loss_function(recon_batch, inputs_full, mu, logvar).item()\n",
    "            samples_cnt += inputs.size(0)\n",
    "\n",
    "            # From BGR -> RGB\n",
    "            perm = torch.LongTensor([2,1,0])\n",
    "            recon_batch = recon_batch[:, perm, :, :]\n",
    "            if reconstr_path != None:\n",
    "                torchvision.utils.save_image(recon_batch, reconstr_path + '/test_' + str(i) + '.png', nrow=nrow)\n",
    "\n",
    "            if i%nReportFreq == 0:\n",
    "                history.append([i, dataloader.batch_size*i, val_loss / samples_cnt])\n",
    "                print('#Batch: {:}, Loss: {:.2f}'.format(i, val_loss / samples_cnt))\n",
    "                \n",
    "    # save log\n",
    "    if csv_log_path != None:\n",
    "        col = [\"#Batch\", \"#Pic\", \"Loss\"]\n",
    "        df_history = pd.DataFrame(data=history, columns=col)\n",
    "        df_history.to_csv(csv_log_path)\n",
    "\n",
    "# Face Generation with Random (or input) latent vector\n",
    "def gen_face(dataloader, model_path, gen_path, nrow=10, conditional=True):\n",
    "    # Load the trained model\n",
    "    cvae = VAE(conditional)\n",
    "    cvae.init_model()\n",
    "    state_dict = torch.load(model_path)\n",
    "    cvae.load_state_dict(state_dict)\n",
    "\n",
    "    # Random sampling\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            if i > 0:\n",
    "                break\n",
    "            _, labels = data\n",
    "            gen_batch = cvae.sampling(c=labels, r=None)\n",
    "\n",
    "            # BGR -> RGB\n",
    "            perm = torch.LongTensor([2,1,0])\n",
    "            full_face_batch = gen_batch[:, perm, :, :]\n",
    "            torchvision.utils.save_image(full_face_batch, gen_path + '/test_' + str(i) + '.png', nrow=nrow, normalize=False, range=(0,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py:117: UserWarning: \n",
      "    Found GPU0 GRID K520 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n",
      "/usr/local/lib/python3.5/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch: 0, #Batch: 0, #Pics:0 Loss: 21919.39\n",
      "#Epoch: 0, #Batch: 20, #Pics:2000 Loss: 20239.55\n",
      "#Epoch: 0, #Batch: 40, #Pics:4000 Loss: 19844.59\n",
      "#Epoch: 0, #Batch: 60, #Pics:6000 Loss: 19646.01\n",
      "#Epoch: 0, #Batch: 80, #Pics:8000 Loss: 19506.06\n",
      "#Epoch: 0, #Batch: 100, #Pics:10000 Loss: 19397.57\n",
      "#Epoch: 0, #Batch: 120, #Pics:12000 Loss: 19318.98\n",
      "#Epoch: 1, #Batch: 0, #Pics:0 Loss: 18782.31\n",
      "#Epoch: 1, #Batch: 20, #Pics:2000 Loss: 18774.26\n",
      "#Epoch: 1, #Batch: 40, #Pics:4000 Loss: 18735.52\n",
      "#Epoch: 1, #Batch: 60, #Pics:6000 Loss: 18690.75\n",
      "#Epoch: 1, #Batch: 80, #Pics:8000 Loss: 18660.97\n",
      "#Epoch: 1, #Batch: 100, #Pics:10000 Loss: 18634.82\n",
      "#Epoch: 1, #Batch: 120, #Pics:12000 Loss: 18606.11\n",
      "#Epoch: 2, #Batch: 0, #Pics:0 Loss: 18479.76\n",
      "#Epoch: 2, #Batch: 20, #Pics:2000 Loss: 18399.30\n",
      "#Epoch: 2, #Batch: 40, #Pics:4000 Loss: 18376.83\n",
      "#Epoch: 2, #Batch: 60, #Pics:6000 Loss: 18358.07\n",
      "#Epoch: 2, #Batch: 80, #Pics:8000 Loss: 18350.37\n",
      "#Epoch: 2, #Batch: 100, #Pics:10000 Loss: 18335.36\n",
      "#Epoch: 2, #Batch: 120, #Pics:12000 Loss: 18322.69\n",
      "#Epoch: 3, #Batch: 0, #Pics:0 Loss: 18154.10\n",
      "#Epoch: 3, #Batch: 20, #Pics:2000 Loss: 18183.97\n",
      "#Epoch: 3, #Batch: 40, #Pics:4000 Loss: 18184.33\n",
      "#Epoch: 3, #Batch: 60, #Pics:6000 Loss: 18170.73\n",
      "#Epoch: 3, #Batch: 80, #Pics:8000 Loss: 18168.57\n",
      "#Epoch: 3, #Batch: 100, #Pics:10000 Loss: 18166.39\n",
      "#Epoch: 3, #Batch: 120, #Pics:12000 Loss: 18157.69\n",
      "#Epoch: 4, #Batch: 0, #Pics:0 Loss: 18011.18\n",
      "#Epoch: 4, #Batch: 20, #Pics:2000 Loss: 18108.25\n",
      "#Epoch: 4, #Batch: 40, #Pics:4000 Loss: 18080.36\n",
      "#Epoch: 4, #Batch: 60, #Pics:6000 Loss: 18075.37\n",
      "#Epoch: 4, #Batch: 80, #Pics:8000 Loss: 18067.87\n",
      "#Epoch: 4, #Batch: 100, #Pics:10000 Loss: 18064.83\n",
      "#Epoch: 4, #Batch: 120, #Pics:12000 Loss: 18059.80\n",
      "#Epoch: 5, #Batch: 0, #Pics:0 Loss: 18071.83\n",
      "#Epoch: 5, #Batch: 20, #Pics:2000 Loss: 18012.12\n",
      "#Epoch: 5, #Batch: 40, #Pics:4000 Loss: 18018.64\n",
      "#Epoch: 5, #Batch: 60, #Pics:6000 Loss: 18014.45\n",
      "#Epoch: 5, #Batch: 80, #Pics:8000 Loss: 18015.55\n",
      "#Epoch: 5, #Batch: 100, #Pics:10000 Loss: 18005.05\n",
      "#Epoch: 5, #Batch: 120, #Pics:12000 Loss: 18001.47\n",
      "#Epoch: 6, #Batch: 0, #Pics:0 Loss: 17954.11\n",
      "#Epoch: 6, #Batch: 20, #Pics:2000 Loss: 17956.90\n",
      "#Epoch: 6, #Batch: 40, #Pics:4000 Loss: 17957.62\n",
      "#Epoch: 6, #Batch: 60, #Pics:6000 Loss: 17973.57\n",
      "#Epoch: 6, #Batch: 80, #Pics:8000 Loss: 17970.23\n",
      "#Epoch: 6, #Batch: 100, #Pics:10000 Loss: 17961.11\n",
      "#Epoch: 6, #Batch: 120, #Pics:12000 Loss: 17958.24\n",
      "#Epoch: 7, #Batch: 0, #Pics:0 Loss: 18056.16\n",
      "#Epoch: 7, #Batch: 20, #Pics:2000 Loss: 17929.13\n",
      "#Epoch: 7, #Batch: 40, #Pics:4000 Loss: 17925.52\n",
      "#Epoch: 7, #Batch: 60, #Pics:6000 Loss: 17922.14\n",
      "#Epoch: 7, #Batch: 80, #Pics:8000 Loss: 17935.02\n",
      "#Epoch: 7, #Batch: 100, #Pics:10000 Loss: 17929.13\n",
      "#Epoch: 7, #Batch: 120, #Pics:12000 Loss: 17929.07\n",
      "#Epoch: 8, #Batch: 0, #Pics:0 Loss: 17736.54\n",
      "#Epoch: 8, #Batch: 20, #Pics:2000 Loss: 17863.13\n",
      "#Epoch: 8, #Batch: 40, #Pics:4000 Loss: 17862.58\n",
      "#Epoch: 8, #Batch: 60, #Pics:6000 Loss: 17881.17\n",
      "#Epoch: 8, #Batch: 80, #Pics:8000 Loss: 17885.65\n",
      "#Epoch: 8, #Batch: 100, #Pics:10000 Loss: 17895.14\n",
      "#Epoch: 8, #Batch: 120, #Pics:12000 Loss: 17898.54\n",
      "#Epoch: 9, #Batch: 0, #Pics:0 Loss: 18063.81\n",
      "#Epoch: 9, #Batch: 20, #Pics:2000 Loss: 17882.60\n",
      "#Epoch: 9, #Batch: 40, #Pics:4000 Loss: 17849.58\n",
      "#Epoch: 9, #Batch: 60, #Pics:6000 Loss: 17859.69\n",
      "#Epoch: 9, #Batch: 80, #Pics:8000 Loss: 17863.60\n",
      "#Epoch: 9, #Batch: 100, #Pics:10000 Loss: 17867.49\n",
      "#Epoch: 9, #Batch: 120, #Pics:12000 Loss: 17877.08\n",
      "#Epoch: 10, #Batch: 0, #Pics:0 Loss: 17924.06\n",
      "#Epoch: 10, #Batch: 20, #Pics:2000 Loss: 17849.68\n",
      "#Epoch: 10, #Batch: 40, #Pics:4000 Loss: 17856.86\n",
      "#Epoch: 10, #Batch: 60, #Pics:6000 Loss: 17859.15\n",
      "#Epoch: 10, #Batch: 80, #Pics:8000 Loss: 17858.57\n",
      "#Epoch: 10, #Batch: 100, #Pics:10000 Loss: 17858.26\n",
      "#Epoch: 10, #Batch: 120, #Pics:12000 Loss: 17859.24\n",
      "#Epoch: 11, #Batch: 0, #Pics:0 Loss: 17808.21\n",
      "#Epoch: 11, #Batch: 20, #Pics:2000 Loss: 17838.36\n",
      "#Epoch: 11, #Batch: 40, #Pics:4000 Loss: 17827.02\n",
      "#Epoch: 11, #Batch: 60, #Pics:6000 Loss: 17831.54\n",
      "#Epoch: 11, #Batch: 80, #Pics:8000 Loss: 17836.58\n",
      "#Epoch: 11, #Batch: 100, #Pics:10000 Loss: 17831.96\n"
     ]
    }
   ],
   "source": [
    "# Data loader\n",
    "x_path = '/root/userspace/project/image_bottom/' # lower-half face\n",
    "c_path = '/root/userspace/project/image_top/' # upper-half face\n",
    "dataloader = set_image_data(x_path, c_path, batch_size=100, max_size=100000)\n",
    "\n",
    "# Train and Save the model\n",
    "conditional = True\n",
    "save_path = '/root/userspace/project/model/model.pth'\n",
    "csv_log_path = '/root/userspace/project/model/model_log.csv'\n",
    "train_and_save(dataloader, model_path=save_path, csv_log_path=csv_log_path, nepoch=30, nReportFreq=20, conditional=True)\n",
    "\n",
    "# Test and Save the pictures\n",
    "reconstr_path = '/root/userspace/project/model/reconstruction/'\n",
    "csv_log_path = None\n",
    "test_and_save(dataloader, model_path=save_path, reconstr_path=reconstr_path, csv_log_path=csv_log_path)\n",
    "\n",
    "# Lower-half face generation\n",
    "gen_path = '/root/userspace/project/model/generation/'\n",
    "gen_face(dataloader, model_path=save_path, gen_path=gen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
