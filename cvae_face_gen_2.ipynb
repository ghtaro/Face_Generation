{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as td\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import cv2\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py:117: UserWarning: \n",
      "    Found GPU0 GRID K520 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Date loader\n",
    "path_image_bottom = '/root/userspace/project/image_bottom/'\n",
    "path_image_top = '/root/userspace/project/image_top/'\n",
    "img_bottom_list = os.listdir(path_image_bottom)\n",
    "img_top_list = os.listdir(path_image_top)\n",
    "\n",
    "# bottom image to nparray\n",
    "max_size = 100000\n",
    "idx = 0\n",
    "img_bottom = []\n",
    "for l in img_bottom_list:\n",
    "    if idx > max_size:\n",
    "        break\n",
    "    p = path_image_bottom + l\n",
    "    im = cv2.imread(p)\n",
    "    img_bottom.append(im)\n",
    "    idx += 1\n",
    "    \n",
    "# top image to nparray\n",
    "idx = 0\n",
    "img_top = []\n",
    "for l in img_top_list:\n",
    "    if idx > max_size:\n",
    "        break\n",
    "    p = path_image_top + l\n",
    "    im = cv2.imread(p)\n",
    "    img_top.append(im)\n",
    "    idx += 1\n",
    "\n",
    "# list to torch tensor\n",
    "# axis change from (n_image, height, width, channel) to (n_image, channel, height, width)\n",
    "# each element is divided by 255 to float\n",
    "x_tensor = torch.from_numpy((np.array(img_bottom) / 255.).astype(np.float32)).to(device)\n",
    "label_tensor = torch.from_numpy((np.array(img_top) / 255.).astype(np.float32)).to(device)\n",
    "x_tensor = x_tensor.permute((0,3,1,2))\n",
    "label_tensor = label_tensor.permute((0,3,1,2))\n",
    "    \n",
    "# Define Data Loader\n",
    "batch_size = 100 # for test\n",
    "trainset = td.TensorDataset(x_tensor, label_tensor) # train data and the label\n",
    "dataloader = td.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE Model components\n",
    "# conv2d: kernel=1->filter size=[1,1], pad=0->padding size=0 (NONE padding),pad=1->ZERO padding for size 1 (for both edges)\n",
    "# why no pooling ??\n",
    "class EncoderModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride, kernel, pad):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=kernel, padding=pad, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, color_channels, pooling_kernels, n_neurons_in_middle_layer):\n",
    "        self.n_neurons_in_middle_layer = n_neurons_in_middle_layer\n",
    "        super().__init__()\n",
    "        # (N,3,50,100)->(N,32,50,100)\n",
    "        self.bottle = EncoderModule(color_channels, 32, stride=1, kernel=1, pad=0)\n",
    "        # (N,32,50,100)->(N,64,50,100)\n",
    "        self.m1 = EncoderModule(32, 64, stride=1, kernel=3, pad=1)\n",
    "        # (N,64,50,100)->(N,128,12,24) with stride size 4. TO BE REVIEWED: pad=0 or p\n",
    "        self.m2 = EncoderModule(64, 128, stride=4, kernel=[3,5], pad=0)\n",
    "        # (N,128,12,24)->(N,256,4,4) with stride size [2,4] TO BE REVIEWED: pad=0 or p\n",
    "        self.m3 = EncoderModule(128, 256, stride=[2,4], kernel=[5,9], pad=0)\n",
    "    def forward(self, x):\n",
    "        out = self.m3(self.m2(self.m1(self.bottle(x))))\n",
    "        return out.view(-1, self.n_neurons_in_middle_layer)\n",
    "\n",
    "class DecoderModule(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride, activation=\"relu\", kernel=1):\n",
    "        super().__init__()\n",
    "        self.convt = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=kernel, stride=stride)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.bn(self.convt(x)))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, color_channels, pooling_kernels, decoder_input_size):\n",
    "        self.decoder_input_size = decoder_input_size\n",
    "        self.in_channels = in_channels\n",
    "        super().__init__()\n",
    "        # (N,128,12,24)<-(N,256,4,4) (in_channels=256 for VAE, 512 for CVAE)\n",
    "        self.m1 = DecoderModule(self.in_channels, 128, stride=[3,6], kernel=[3,6])\n",
    "        # (N,64,50,100)<-(N,128,12,24)\n",
    "        self.m2 = DecoderModule(128, 64, stride=[4,4], kernel=[6,8])\n",
    "        # (N,32,50,100)<-(N,64,50,100)\n",
    "        self.m3 = DecoderModule(64, 32, stride=1, kernel=1)\n",
    "        # (N,3,50,100)<-(N,32,50,100)\n",
    "        self.bottle = DecoderModule(32, color_channels, stride=1, activation=\"sigmoid\", kernel=1)\n",
    "    def forward(self, x):\n",
    "        # support 2 cases\n",
    "        # VAE: input size (N,256*4*4) ->(N,256,4,4)\n",
    "        # CVAE: input size (N,2*256*4*4) ->(N,512,4,4)\n",
    "        out = x.view(-1, self.in_channels, self.decoder_input_size, self.decoder_input_size)\n",
    "        out = self.m3(self.m2(self.m1(out)))\n",
    "        return self.bottle(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CVAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, conditional):\n",
    "        self.device = device\n",
    "        self.conditional = conditional # if true, CVAE\n",
    "\n",
    "        super().__init__()\n",
    "        # resolution\n",
    "        # cifar : 32 -> 8 -> 4\n",
    "        pooling_kernel = [1, 1]\n",
    "        encoder_output_size = 4 # TO BE REVIEWED: would be not nice... (should be automatically linked to the Encoder architecture)\n",
    "        color_channels = 3\n",
    "\n",
    "        # Middle\n",
    "        # output size [N,256,4,4]\n",
    "        self.n_latent_features = 64 # dimension of z\n",
    "        encoder_1d_output_size = 256 * encoder_output_size * encoder_output_size\n",
    "        n_neurons_middle_layer = (2 if self.conditional else 1) * encoder_1d_output_size # we double if CVAE\n",
    "\n",
    "        # Encoder for X (lower-half face)\n",
    "        self.encoder_x = Encoder(color_channels, pooling_kernel, encoder_1d_output_size)        \n",
    "        # Encoder for Y (Upper-half face, \"label\")\n",
    "        if self.conditional:\n",
    "            self.encoder_y = Encoder(color_channels, pooling_kernel, encoder_1d_output_size) # ALMOST 100% SURE ABOUT THIS SETTING, BUT TO BE REVIEWED\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_neurons_middle_layer, self.n_latent_features) # for encoder X\n",
    "        self.fc2 = nn.Linear(n_neurons_middle_layer, self.n_latent_features) # for encoder X\n",
    "        self.fc3 = nn.Linear(self.n_latent_features, encoder_1d_output_size) # for decoder X\n",
    "\n",
    "        if self.conditional:\n",
    "            self.fc4 = nn.Linear(encoder_1d_output_size, self.n_latent_features) # for encoder Y\n",
    "            self.fc5 = nn.Linear(encoder_1d_output_size, self.n_latent_features) # for encoder Y\n",
    "            self.fc6 = nn.Linear(self.n_latent_features, encoder_1d_output_size) # for decoder Y\n",
    "\n",
    "        # Decoder for X (Labels should be taken into account also for this decoder stage...)\n",
    "        self.decoder_x = Decoder(256*(2 if self.conditional else 1), color_channels, pooling_kernel, encoder_output_size)\n",
    "        # Decoder for Y (Upper-half face, \"label\")\n",
    "        if self.conditional:\n",
    "            self.decoder_y = Decoder(256, color_channels, pooling_kernel, encoder_output_size)\n",
    "\n",
    "    def _reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn(*mu.size()).to(self.device)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def _bottleneck_x(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def _bottleneck_y(self, h):\n",
    "        mu, logvar = self.fc4(h), self.fc5(h)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def sampling(self, y=None, r=None):\n",
    "        # assume latent features space ~ N(0, 1)\n",
    "        n = y.size(0) if self.conditional else 64 # number of faces\n",
    "        if r==None:\n",
    "            zx = torch.randn(n, self.n_latent_features).to(self.device)\n",
    "        else:\n",
    "            zx = torch.full((n,self.n_latent_features), fill_value=r).to(self.device)\n",
    "        zx = self.fc3(zx) # 256 * encode_out_size(4)* encode_out_size(4)\n",
    "        \n",
    "        # concat\n",
    "        if self.conditional:\n",
    "            hy = self.encoder_y(y) # 256 * encode_out_size(4)* encode_out_size(4)\n",
    "            z = torch.cat((zx, hy), dim=-1) # 2* 256 * encode_out_size(4)* encode_out_size(4)\n",
    "        else:\n",
    "            z = zx\n",
    "        # decode\n",
    "        dx = self.decoder_x(z)\n",
    "        d = torch.cat((y,dx), dim=2)\n",
    "        return d\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # Encoder for X and for Y\n",
    "        # output size [N,256*4*4]\n",
    "        hx = self.encoder_x(x)\n",
    "        if self.conditional:\n",
    "            hy = self.encoder_y(y)\n",
    "        # concat if CVAE\n",
    "        if self.conditional:\n",
    "            h = torch.cat((hx, hy), dim=-1)\n",
    "        else:\n",
    "            h = hx\n",
    "        # Bottle-neck\n",
    "        # z,mu,logvar size [N,256*4*4]->[N,64]\n",
    "        zx, mu_x, logvar_x = self._bottleneck_x(h)\n",
    "        if self.conditional:\n",
    "            zy, mu_y, logvar_y = self._bottleneck_y(hy)\n",
    "            \n",
    "        # decoder\n",
    "        # output size [N,64]->[N,256*4*4]\n",
    "        zx = self.fc3(zx)\n",
    "        if self.conditional:\n",
    "            zx = torch.cat((zx, hy), dim=-1) # 2* 256 * encode_out_size(4)* encode_out_size(4)\n",
    "            zy = self.fc6(zy)\n",
    "            \n",
    "        dx = self.decoder_x(zx)\n",
    "        if self.conditional:\n",
    "            dy = self.decoder_y(zy)\n",
    "            d = torch.cat((dy,dx), dim=2)\n",
    "            mu = torch.sqrt(mu_x.pow(2) + mu_y.pow(2))\n",
    "            logvar = torch.sqrt(logvar_x.pow(2) + logvar_y.pow(2))\n",
    "        else:\n",
    "            d = dx\n",
    "            mu = mu_x\n",
    "            logvar = logvar_x\n",
    "        return d, mu, logvar\n",
    "\n",
    "    # Model\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        # https://arxiv.org/abs/1312.6114 (Appendix B)\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)        \n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + KLD\n",
    "\n",
    "    def init_model(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self = self.cuda()\n",
    "            torch.backends.cudnn.benchmark=True\n",
    "        self.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bbe762ff20be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Init CVAE model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconditional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VAE' is not defined"
     ]
    }
   ],
   "source": [
    "# Init CVAE model\n",
    "conditional = True\n",
    "cvae = VAE(conditional)\n",
    "cvae.init_model()\n",
    "\n",
    "# Train\n",
    "cvae.train()\n",
    "nepoch = 20\n",
    "nReportFreq = 20\n",
    "for epoch in range(nepoch):\n",
    "    train_loss = 0\n",
    "    samples_cnt = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Extract input (lower-half face) and label (upper-half face)\n",
    "        inputs, labels = data\n",
    "        # Define them as torch Variable (auto-gradient will be applied on)\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # init the optimiser (FORGOT why we initialise the grad at the beginning of each epoch)\n",
    "        cvae.optimizer.zero_grad()\n",
    "        # forward\n",
    "        recon_batch, mu, logvar = cvae.forward(inputs, labels)\n",
    "        # evaluate loss\n",
    "        inputs_full = torch.cat((labels, inputs), dim=2)\n",
    "        loss = cvae.loss_function(recon_batch, inputs_full, mu, logvar)\n",
    "        # back prop and update the weights\n",
    "        loss.backward()\n",
    "        cvae.optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        samples_cnt += inputs.size(0)\n",
    "        \n",
    "        if i%nReportFreq == 0:\n",
    "            print('#Epoch: {:}, #Batch: {:}, #Pics:{:} Loss: {:.2f}'.format(epoch, i, batch_size*i, train_loss / samples_cnt))\n",
    "        \n",
    "# save model\n",
    "state_dict = cvae.state_dict()\n",
    "torch.save(state_dict, '/root/userspace/project/model2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Batch: 0, Loss: 18919.40\n",
      "#Batch: 20, Loss: 18858.75\n",
      "#Batch: 40, Loss: 18862.76\n",
      "#Batch: 60, Loss: 18865.66\n",
      "#Batch: 80, Loss: 18872.17\n",
      "#Batch: 100, Loss: 18867.90\n",
      "#Batch: 120, Loss: 18865.80\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# 学習済みパラメータの読み込み\n",
    "conditional = True\n",
    "cvae = VAE(conditional)\n",
    "cvae.init_model()\n",
    "state_dict = torch.load('/root/userspace/project/model2.pth')\n",
    "cvae.load_state_dict(state_dict)\n",
    "\n",
    "cvae.eval()\n",
    "val_loss = 0\n",
    "samples_cnt = 0\n",
    "nReportFreq = 20\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Extract input (lower-half face) and label (upper-half face)\n",
    "        inputs, labels = data\n",
    "        # Define them as torch Variable (auto-gradient will be applied on)\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        recon_batch, mu, logvar = cvae.forward(inputs, labels)\n",
    "        inputs_full = torch.cat((labels, inputs), dim=2)\n",
    "        val_loss += cvae.loss_function(recon_batch, inputs_full, mu, logvar).item()\n",
    "        samples_cnt += inputs.size(0)\n",
    "\n",
    "        # From BGR -> RGB\n",
    "        perm = torch.LongTensor([2,1,0])\n",
    "        recon_batch = recon_batch[:, perm, :, :]\n",
    "        torchvision.utils.save_image(recon_batch, '/root/userspace/project/reconstruction2/test_'+str(i)+'.png', nrow=10)\n",
    "\n",
    "        if i%nReportFreq == 0:\n",
    "            print('#Batch: {:}, Loss: {:.2f}'.format(i, val_loss / samples_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloader):\n",
    "        if i > 0:\n",
    "            break\n",
    "        _, labels = data\n",
    "        gen_batch = cvae.sampling(y=labels, r=None)\n",
    "\n",
    "        # BGR -> RGB\n",
    "        perm = torch.LongTensor([2,1,0])\n",
    "        full_face_batch = gen_batch[:, perm, :, :]\n",
    "        torchvision.utils.save_image(full_face_batch, '/root/userspace/project/generation2/test_'+str(i)+'.png', nrow=10, normalize=False, range=(0,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
